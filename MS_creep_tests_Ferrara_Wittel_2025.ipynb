{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental decomposition scheme for mechanosoprtive strain calculation\n",
    "#### *A. Ferrara and F.K. Wittel \"Mechanosorptive Creep of Norway Spruce on the Tissue Scale Perpendicular to Grain\"*, *Holzforschung* (2025)\n",
    "\n",
    "This Notebook allows to plot the data contained in **_MS_Creep_Tests_Dataset_** ([10.17632/rsrsw8h7mv.1]()). The dataset contains experimental results from our study (Ferrara and Wittel, 2025b), in which **mechanosorptive creep** tests were performed on **Norway spruce tissue slices** in the transverse anatomical directions, radial (R) and tangential (T), under cyclic relative humidity (RH) between 30% and 90%. Samples were cut in the orientations **{RL, RT, TR}**, where the first letter denotes the longitudinal dimension and the second the transverse width, all comprising alternating bands of earlywood (EW) and latewood (LW). Tests were carried out at **different loading degrees (LD)**: {RL, RT} at 30% and 50%, and {TR} at 40% of the respective tensile strength. Reference tensile strengths were taken from our earlier work (Ferrara and Wittel, 2024).\n",
    "\n",
    "The dataset is organized by sample type, with each file containing multiple sheets corresponding to individual experiments. Each sheet reports detailed information about the sample (e.g., dimensions, loading degree, etc.), together with time, moisture content, and mechanosorptive creep strain and compliance. The mechanosorptive creep strain was calculated by applying the proposed incremental decomposition scheme to isolate them from the total strain directly measured with Digital Image Correlation (DIC).\n",
    "\n",
    "In addition, The dataset includes two CSV files with the results of:  \n",
    "- **Tensile elastic tests** on Norway spruce tissue slices, provided as mean elastic compliance values from our earlier work (Ferrara and Wittel, 2024), enabling analyses of how creep compliance scales with elastic compliance  \n",
    "- **Tensile creep tests**, provided as element compliances of the Kelvin–Voigt model describing the mean viscoelastic compliances from our previous work (Ferrara and Wittel, 2025a), which serve as input for calculating the viscoelastic strain component  \n",
    "and one NPZ file with the results of\n",
    "- **Dynamic Vapor Sorption (DVS) tests** on a RL-slice, following the RH profile applied in the mechanosorptive experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and working path\n",
    "Run the following section to import the required libraries and download the data folder in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import requests\n",
    "import zipfile\n",
    "from math import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "from scipy.optimize import lsq_linear\n",
    "from scipy.optimize import least_squares\n",
    "from scipy.stats import trim_mean\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Download dataset in the current folder\n",
    "url = \"https://data.mendeley.com/public-files/datasets/rsrsw8h7mv/files/c74b8815-a92e-4921-975b-47f9dcebeb3d/file_downloaded\" # Link to dataset folder\n",
    "extraction_path = os.getcwd()  # path to current folder\n",
    "zip_file_path = os.path.join(extraction_path, \"downloaded_folder.zip\")  # path to folder where the zip file will be saved\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\"} # get headers (optional, but recommended for some sites)\n",
    "response = requests.get(url, headers=headers) # get response from URL\n",
    "if response.status_code == 200:\n",
    "    with open(zip_file_path, \"wb\") as f:  # download the zip file\n",
    "        f.write(response.content)   \n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:  # unzip the file\n",
    "            zip_ref.extractall(extraction_path)\n",
    "        extracted_file = zip_ref.namelist()[0]  # name of the extracted folder\n",
    "        folder_path = os.path.join(extraction_path, extracted_file) # path to dataset folder\n",
    "        print(f\"Working in: {folder_path}\")\n",
    "        os.remove(zip_file_path)  # remove the downloaded zip file\n",
    "    except zipfile.BadZipFile:\n",
    "        print(\"Error: The file is not a zip file or it is corrupted.\")\n",
    "else:\n",
    "    print(f\"Failed to download. Status code: {response.status_code}\")\n",
    "\n",
    "# Set file paths\n",
    "dvs_path = folder_path + 'dvs_data.npz'\n",
    "elastic_path = folder_path + 'elastic_compliances_Ferrara_Wittel_2024.csv'\n",
    "viscoel_path = folder_path + 'master_vec_prony_param.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized functions\n",
    "Run the following section to make the custom functions available for use in the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve experiment datasets\n",
    "def read_sheet_meta_and_data(xlsx_path, sheet_name):\n",
    "    df_all = pd.read_excel(xlsx_path, sheet_name=sheet_name, header=None)\n",
    "    # Find header row\n",
    "    header_row = None\n",
    "    for i in range(len(df_all)):\n",
    "        v = str(df_all.iloc[i, 0]) if pd.notna(df_all.iloc[i, 0]) else \"\"\n",
    "        if v.strip() == \"Time [h]\":\n",
    "            header_row = i\n",
    "            break\n",
    "    if header_row is None:\n",
    "        raise ValueError(f'Could not find \"Time [h]\" header in {os.path.basename(xlsx_path)}:{sheet_name}')\n",
    "\n",
    "    # Meta above header (skip the blank row just before header)\n",
    "    meta_pairs = df_all.iloc[:header_row-1, :2].dropna(how='all')\n",
    "    meta = {}\n",
    "    for _, r in meta_pairs.iterrows():\n",
    "        k = r.iloc[0]\n",
    "        v = r.iloc[1]\n",
    "        if pd.isna(k):\n",
    "            continue\n",
    "        k = str(k)\n",
    "        if k not in meta and not (isinstance(v, float) and np.isnan(v)):\n",
    "            meta[k] = v\n",
    "\n",
    "    # Data table from header_row\n",
    "    data_tbl = pd.read_excel(xlsx_path, sheet_name=sheet_name, header=header_row)\n",
    "    return meta, data_tbl\n",
    "# end: def read_sheet_meta_and_data\n",
    "\n",
    "# Extract KV-compliances from excel\n",
    "def extract_comp(meta):\n",
    "    c1 = float(meta.get('Compliance_1 [1/GPa]', np.nan))/1000\n",
    "    c2 = float(meta.get('Compliance_2 [1/GPa]', np.nan))/1000\n",
    "    c3 = float(meta.get('Compliance_3 [1/GPa]', np.nan))/1000\n",
    "    return np.array([c1, c2, c3], dtype=float)\n",
    "# end: def extract_comp\n",
    "\n",
    "# Extract loading degree from excel\n",
    "def ld_from_meta(meta):\n",
    "    val = meta.get('Loading Degree [%]', meta.get('Nominal Loading Degree [-]', np.nan))\n",
    "    try:\n",
    "        v = float(val)\n",
    "        return v/100.0 if v > 1.5 else v\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "# end: def ld_from_meta\n",
    "\n",
    "# Generate stress array\n",
    "def stress_series(meta, data_tbl, t):\n",
    "    if 'Stress [MPa]' in data_tbl.columns:\n",
    "        s = data_tbl['Stress [MPa]'].to_numpy(dtype=float)\n",
    "        if len(s) == len(t):\n",
    "            return s\n",
    "    s0 = float(meta.get('Creep Stress [MPa]', np.nan))\n",
    "    return np.full(len(t), s0, dtype=float)\n",
    "# end: def stress_series\n",
    "\n",
    "# Group colors \n",
    "def group_and_colors(entries, cmap_dict):\n",
    "    colors_by_ld, _ = build_ld_colormaps(entries, cmap_dict)\n",
    "    grouped = defaultdict(list)\n",
    "    for e in entries:\n",
    "        grouped[e['ld']].append(e)\n",
    "    return grouped, colors_by_ld\n",
    "# end: def group_and_colors\n",
    "\n",
    "# Build moisture cycles with different first sorption with corresponding time and stress\n",
    "def build_cycles_with_initial(tdata0, wdata0, tdata, wdata, total_cycles, stress_value, stressed_cycles):\n",
    "    \n",
    "    # Convert to arrays\n",
    "    t0 = np.asarray(tdata0)\n",
    "    w0 = np.asarray(wdata0)\n",
    "    t  = np.asarray(tdata)\n",
    "    w  = np.asarray(wdata)\n",
    "\n",
    "    # Determine cycle period\n",
    "    dt = t[-1] - t[-2]\n",
    "    period = (t[-1] - t[0]) + dt\n",
    "\n",
    "    # Total length\n",
    "    n0 = t0.size\n",
    "    n  = t.size\n",
    "    N  = n0 + n*(total_cycles-1)\n",
    "    # Pre-allocate\n",
    "    t_full = np.empty(N, dtype=t.dtype)\n",
    "    w_full = np.empty(N, dtype=w.dtype)\n",
    "    s_full = np.empty(N, dtype=float)\n",
    "\n",
    "    # Set first cycle\n",
    "    t_full[:n0] = t0\n",
    "    w_full[:n0] = w0\n",
    "    s_full[:n0] = stress_value if 0 < stressed_cycles else 0.0\n",
    "    # Set remaining cycles\n",
    "    for i in range(1, total_cycles):\n",
    "        start = n0 + (i-1)*n\n",
    "        end   = start + n\n",
    "        # Time shift\n",
    "        t_full[start:end] = t + (i-1)*period\n",
    "        w_full[start:end] = w\n",
    "        # Apply stress for initial cycles, else 0\n",
    "        if i < stressed_cycles:\n",
    "            s_full[start:end] = stress_value\n",
    "        elif i == stressed_cycles:\n",
    "            s_full[start-1] = 0.0\n",
    "            s_full[start:end] = 0.0\n",
    "        else:\n",
    "            s_full[start:end] = 0.0\n",
    "\n",
    "    return t_full, w_full, s_full\n",
    "# end: build_cycles_with_initial\n",
    "\n",
    "# Convert RH into mean w (mean S/D of tissues from standard dvs test)\n",
    "def RH_to_w_mean(x):\n",
    "    w = 7.155e-11 * x**5 - 1.659e-08 * x**4 + 1.75e-06 * x**3 - 9.343e-05 * x**2 + 0.003795 * x + 0.002295\n",
    "    return w\n",
    "# end: RH_to_w_mean\n",
    "\n",
    "# Fit elastic compliances to moisture\n",
    "def fit_elastic_comp(elastic_path):\n",
    "\n",
    "    # Read csv file\n",
    "    df = pd.read_csv(elastic_path)\n",
    "    # Group and compute mean compliance\n",
    "    mean_df = df.rename(columns={'1/C0 [1/Mpa]': 'C0_mean'})\n",
    "    # Calculate average w for each RH\n",
    "    mean_df['w'] = RH_to_w_mean(mean_df['RH'])\n",
    "    # Fit quadratic function in w\n",
    "    el_fits = {}\n",
    "    for stype, sub in mean_df.groupby('sample_type'):\n",
    "        x = sub['w'].values\n",
    "        y = sub['C0_mean'].values\n",
    "        A = np.vstack(( x**2, x, np.ones_like(x) )).T\n",
    "        # choose bounds per sample_type\n",
    "        if stype in (\"LR\", \"LT-LW\"):\n",
    "            lower = [-np.inf, 0.0,     0.0]\n",
    "            upper = [ 0.0,     np.inf, np.inf]\n",
    "        else:\n",
    "            lower = [ 0.0,     0.0,     0.0]\n",
    "            upper = [ np.inf, np.inf, np.inf]\n",
    "        # Solve\n",
    "        res = lsq_linear(A, y, bounds=(lower, upper))\n",
    "        a, b, c = res.x\n",
    "        el_fits[stype] = (a, b, c)\n",
    "\n",
    "    return el_fits\n",
    "# end: fit_elastic_comp\n",
    "\n",
    "# Fitting model of mechanosorptive strain\n",
    "def mechanosorptive_model(comp_j, stress, tdata, wdata):\n",
    "    \n",
    "    # Charachteristic moistures [-]\n",
    "    mu_0 = np.array([1., 10., 100.])/100.\n",
    "    # Build moisture‐rate array\n",
    "    tdata = np.asarray(tdata)\n",
    "    wdata = np.asarray(wdata)\n",
    "    dt = np.diff(tdata)\n",
    "    dw = np.diff(wdata)\n",
    "    wrate = np.concatenate(([0.], np.abs(dw) / dt))\n",
    "    \n",
    "    # Initialize\n",
    "    eps_j  = np.zeros_like(mu_0)\n",
    "    eps_ms = np.zeros_like(wrate)\n",
    "\n",
    "    # Calculate mechanosorptive strain\n",
    "    for i in range(1, len(tdata)):\n",
    "        delta_t = tdata[i] - tdata[i-1]\n",
    "        # Calculate mechanosorptive increment\n",
    "        deps = (wrate[i] / mu_0) * (comp_j * stress[i] - eps_j)\n",
    "        eps_j  += deps * delta_t\n",
    "        eps_ms[i] = eps_j.sum()\n",
    "    \n",
    "    return eps_ms\n",
    "# end: def mechanosorptive_model\n",
    "\n",
    "# Build colormaps for loading degrees\n",
    "def build_ld_colormaps(data, cmap_dict, scalar_range=(0.4, 0.9)):\n",
    "\n",
    "    if cmap_dict is None: cmap_dict = {0.5: cm.Purples, 0.4: cm.Greens, 0.3: cm.Oranges}\n",
    "    fallback_cmap = cm.Greys\n",
    "\n",
    "    # Group datasets by LD\n",
    "    grouped = defaultdict(list)\n",
    "    for entry in data: grouped[entry['ld']].append(entry)\n",
    "\n",
    "    # Generate list of colors for each group\n",
    "    start, end = scalar_range\n",
    "    colors_by_ld = {}\n",
    "    rep_colors = {}\n",
    "\n",
    "    # Sort by LD for deterministic behavior\n",
    "    for ld_value in sorted(grouped.keys()):\n",
    "        entries = grouped[ld_value]\n",
    "        cmap = cmap_dict.get(ld_value, fallback_cmap)\n",
    "        n = len(entries)\n",
    "\n",
    "        if n > 1: scalars = np.linspace(start, end, n)\n",
    "        else: scalars = [(start + end) / 2.0]\n",
    "\n",
    "        cols = [cmap(s) for s in scalars]\n",
    "        colors_by_ld[ld_value] = cols\n",
    "\n",
    "        # Choose a representative color:\n",
    "        # - third from the end if available, else the last available\n",
    "        rep_idx = n - 3 if n >= 3 else n - 1\n",
    "        rep_colors[ld_value] = cols[rep_idx]\n",
    "\n",
    "    return colors_by_ld, rep_colors\n",
    "# end: def build_ld_colormaps\n",
    "\n",
    "# Set shared grid between primary and secondary y-axis\n",
    "def set_shared_grid(ax1, ax2):\n",
    "\n",
    "    # Grab the current primary ticks\n",
    "    primary_ticks = ax1.get_yticks()\n",
    "    # Keep the same gridlines at the primary_ticks\n",
    "    ax1.set_yticks(primary_ticks)\n",
    "    # Compute secondary ticks by slicing its data‐range into len(primary_ticks) points\n",
    "    smin, smax = ax2.get_ylim()\n",
    "    secondary_ticks = np.linspace(smin, smax, len(primary_ticks))\n",
    "    ax2.set_yticks(secondary_ticks)\n",
    "    ax2.set_yticklabels([str(round(t,3)) for t in secondary_ticks])\n",
    "\n",
    "    return\n",
    "# end: def set_shared_grid\n",
    "\n",
    "# Customize plot\n",
    "def customize_plot(ax1, ax2, fontsize):\n",
    "    ax1.grid(True, alpha=0.5)\n",
    "    # Customize axis\n",
    "    ax1.set_xlabel('t [h]', fontsize=fontsize)\n",
    "    ax1.set_xlim(left=0)\n",
    "    ax1.set_ylabel(r'$\\varepsilon$ [-]', fontsize=fontsize)\n",
    "    ax1.set_ylim(bottom=0)\n",
    "    ax1.tick_params(axis='both', labelsize=fontsize)\n",
    "    ax2.tick_params(axis='both', labelsize=fontsize)\n",
    "    return ax1, ax2\n",
    "# end: customize_plot\n",
    "\n",
    "# Set same number of thicks on twin axes\n",
    "def match_tick_count(ax_src, ax_tgt, axis='y'):\n",
    "    if axis not in ('x','y'):\n",
    "        raise ValueError(\"`axis` must be 'x' or 'y'\")\n",
    "\n",
    "    # Pick off the existing tick *locations* on the source axis\n",
    "    if axis == 'x':\n",
    "        n = len(ax_src.get_xticks())-1\n",
    "        locator = mticker.MaxNLocator(n)\n",
    "        ax_tgt.xaxis.set_major_locator(locator)\n",
    "    else:\n",
    "        n = len(ax_src.get_yticks())-1\n",
    "        locator = mticker.MaxNLocator(n)\n",
    "        ax_tgt.yaxis.set_major_locator(locator)\n",
    "    return\n",
    "# end: def match_tick_count\n",
    "\n",
    "# Set same tick values on twin axes\n",
    "def match_tick_values(ax_src, ax_tgt, axis='y', copy_labels=True, copy_limits=True):\n",
    "    if axis not in ('x','y'):\n",
    "        raise ValueError(\"`axis` must be 'x' or 'y'\")\n",
    "    # 1) Optionally copy axis limits\n",
    "    if copy_limits:\n",
    "        if axis == 'y':\n",
    "            ax_tgt.set_ylim(ax_src.get_ylim())\n",
    "        else:\n",
    "            ax_tgt.set_xlim(ax_src.get_xlim())\n",
    "\n",
    "    # 2) Grab the *exact* tick positions\n",
    "    if axis == 'y':\n",
    "        ticks = ax_src.get_yticks()\n",
    "        # use a FixedLocator so Matplotlib doesn't try to autoscale them away\n",
    "        ax_tgt.yaxis.set_major_locator(mticker.FixedLocator(ticks))\n",
    "        if copy_labels:\n",
    "            labels = [t.get_text() for t in ax_src.get_yticklabels()]\n",
    "            ax_tgt.set_yticklabels(labels)\n",
    "    else:\n",
    "        ticks = ax_src.get_xticks()\n",
    "        ax_tgt.xaxis.set_major_locator(mticker.FixedLocator(ticks))\n",
    "        if copy_labels:\n",
    "            labels = [t.get_text() for t in ax_src.get_xticklabels()]\n",
    "            ax_tgt.set_xticklabels(labels)\n",
    "    return\n",
    "# end: def match_tick_values\n",
    "\n",
    "# Detect spikes in strain rate\n",
    "def detect_single_point_spikes(y, k=5.0, eps=1e-12):\n",
    "    y = np.asarray(y, float)\n",
    "    dl = np.abs(y[1:-1] - y[:-2])\n",
    "    dr = np.abs(y[1:-1] - y[2:])\n",
    "    diff = np.diff(y)\n",
    "    med = np.median(np.abs(diff - np.median(diff))) + eps\n",
    "    scale = 1.4826 * med\n",
    "    same_side = np.sign(y[1:-1] - y[:-2]) == np.sign(y[1:-1] - y[2:])\n",
    "    spikes_mid = (dl > k*scale) & (dr > k*scale) & same_side\n",
    "    idx = np.where(spikes_mid)[0] + 1\n",
    "    return idx\n",
    "# end: def detect_single_point_spikes\n",
    "\n",
    "# Smooth spikes in strain rate\n",
    "def fix_spikes_linear(y, idx):\n",
    "    y = np.asarray(y, float).copy()\n",
    "    for i in idx:\n",
    "        if 0 < i < len(y)-1:\n",
    "            y[i] = 0.5*(y[i-1] + y[i+1])  # interp lineare\n",
    "    return y\n",
    "# end: def fix_spikes_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load experimental datasets\n",
    "Run the following section to load the datasets from the DVS test and retrieve all datasets corresponding to the available sample types **{RL, RT, TR}**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## PREPARE EXPERIMENTAL DATASETS ##################\n",
    "# Load data from dvs test\n",
    "data = np.load(dvs_path)\n",
    "wdata0 = data['w'][0] # moisture of 1st cycle\n",
    "tdata0 = data['t'][0] # time of 1st cycle\n",
    "wdata = data['w'][1] # moisture of following cycles\n",
    "tdata = data['t'][1]  # time of following cycles\n",
    "\n",
    "# Scan Excel workbooks\n",
    "excel_files = sorted(glob.glob(os.path.join(folder_path, \"*_samples_results.xlsx\")))\n",
    "if not excel_files:\n",
    "    raise FileNotFoundError(f\"No Excel workbooks '*_samples_results.xlsx' found in {folder_path}\")\n",
    "\n",
    "# Derive sample_types from filenames\n",
    "sample_types = [os.path.basename(p).split('_')[0] for p in excel_files]\n",
    "\n",
    "# Retrieve datasets\n",
    "dataset = []\n",
    "for xlsx_path, sample_type in zip(excel_files, sample_types):\n",
    "    xl = pd.ExcelFile(xlsx_path)\n",
    "    for sheet_name in xl.sheet_names:\n",
    "        meta, df = read_sheet_meta_and_data(xlsx_path, sheet_name)\n",
    "\n",
    "        # Extract table columns\n",
    "        t_full = df['Time [h]'].to_numpy(dtype=float)\n",
    "        w_full = (df['Moisture [%]'].to_numpy(dtype=float)/100.0\n",
    "                  if 'Moisture [%]' in df.columns else np.full(len(t_full), np.nan))\n",
    "        eps_full = df['MS Creep Strain [-]'].to_numpy(dtype=float)\n",
    "        s_full = stress_series(meta, df, t_full)\n",
    "\n",
    "        # Extract basic fields\n",
    "        ld = ld_from_meta(meta)\n",
    "        sample_name = str(meta.get('Sample', sheet_name))\n",
    "        exp = meta.get('Experiment', sample_name)\n",
    "\n",
    "        # Extract meta dict\n",
    "        meta_out = {\n",
    "            'thickness'   : float(meta.get('Thickness [mm]', np.nan)),\n",
    "            'width'       : float(meta.get('Width [mm]', np.nan)),\n",
    "            'area'        : float(meta.get('Area [mm^2]', np.nan)),\n",
    "            'exp_duration': float(meta.get('Duration [h]', t_full[-1] if len(t_full) else np.nan)),\n",
    "            'load_nom'    : float(meta.get('Nominal Load [N]', np.nan)),\n",
    "            'stress_nom'  : float(meta.get('Nominal Stress [MPa]', np.nan)),\n",
    "            'load_meas'   : float(meta.get('Creep Load [N]', np.nan)),\n",
    "            'stress_meas' : float(meta.get('Creep Stress [MPa]', np.nan)),\n",
    "            'loading_deg' : float(meta.get('Loading Degree [%]', np.nan))/100.0 if 'Loading Degree [%]' in meta else float(meta.get('Nominal Loading Degree [-]', np.nan)),\n",
    "            'comp_opt'    : extract_comp(meta),\n",
    "        }\n",
    "\n",
    "        # Store data\n",
    "        dataset.append({\n",
    "            'exp'            : exp,\n",
    "            'sample_type'    : sample_type,\n",
    "            'sample_name'    : sample_name,\n",
    "            'ld'             : ld,\n",
    "            't_full'         : t_full,\n",
    "            'w_full'         : w_full,\n",
    "            's_full'         : s_full,\n",
    "            'eps_full'       : eps_full,\n",
    "            'stress'         : s_full,\n",
    "            'meta'           : meta_out,\n",
    "        })\n",
    "\n",
    "print(f\"Datasets for samples {', '.join(sample_types)} have been retrieved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot mechanosorptive strains, compliances, and normalized compliances\n",
    "\n",
    "Run the following section to plot the results of the mechanosorptive tests, grouped by **sample type** **{RL, RT, TR}** and **loading degree (LD)**:\n",
    "\n",
    "- Extracted mechanosorptive strains $\\varepsilon^{ms}$, together with moisture cycles $\\omega$  \n",
    "- Kelvin–Voigt fits of $\\varepsilon^{ms}$ with the trimmed-mean fit per LD\n",
    "- Mechanosorptive compliances $M_c$ computed from the fitted strains, with the trimmed-mean compliance per LD\n",
    "- Normalized mechanosorptive compliances $\\overline{M_c}/C_0^{-1}$, obtained by dividing the mechanosorptive compliance by the elastic compliance at a reference moisture content (set as input between 0 and 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## PLOT MS RESULTS ##################\n",
    "# Set layout variables\n",
    "fontsize = 28\n",
    "linewidth = 2\n",
    "ylim_row1 = 0.04 # eps [-]\n",
    "ylim_row2 = ylim_row1\n",
    "ylim_row3 = 24 # Mc [1/GPa]\n",
    "ylim_row4 = 9 # normalized Mc [-]\n",
    "xlim = 21.5 # time [h]\n",
    "w_ref = 0.08 # reference moisture content [-] for elastic compliance in normalized compliance\n",
    "\n",
    "# Generate colormap\n",
    "cmap_dict = {0.5: cm.Purples, 0.4: cm.Greens, 0.3: cm.Oranges}\n",
    "dataset_norm = []\n",
    "for e in dataset:\n",
    "    e2 = e.copy()\n",
    "    e2['ld'] = float(np.round(e['ld'], 1)) if e.get('ld') is not None else e.get('ld')\n",
    "    dataset_norm.append(e2)\n",
    "_, rep_colors = build_ld_colormaps(dataset_norm, cmap_dict)\n",
    "\n",
    "# Retrive elastic compliance fit\n",
    "comp_el = fit_elastic_comp(elastic_path)  # dict: sample_type -> (a,b,c)\n",
    "\n",
    "# Prepare plot\n",
    "fig, axes = plt.subplots(4, len(sample_types), figsize=(7 * len(sample_types), 6 * 4), constrained_layout=True)\n",
    "ax_leg = None\n",
    "last_ax2 = None\n",
    "\n",
    "for row in range(4):\n",
    "    for col, sample_type in enumerate(sample_types):\n",
    "        ax1 = axes[row, col]\n",
    "        ax2 = ax1.twinx()\n",
    "        last_ax2 = ax2\n",
    "    \n",
    "        # Subset for this sample_type\n",
    "        entries = [e for e in dataset_norm if e['sample_type'] == sample_type]\n",
    "        grouped, colors_by_ld = group_and_colors(entries, cmap_dict)\n",
    "\n",
    "        for ld_val, group in grouped.items():\n",
    "            eps_fit_list = []\n",
    "            s_list = []\n",
    "            for entry, color in zip(group, colors_by_ld[ld_val]):\n",
    "                # Retrieve datasets\n",
    "                w_full = entry['w_full'] # [-]\n",
    "                t_full = entry['t_full'] # [h]\n",
    "                s_full = entry['s_full'] # [MPa]\n",
    "                eps_full = entry['eps_full'] # [-]\n",
    "                comp_opt = entry['meta'].get('comp_opt', None)\n",
    "\n",
    "                # Customize plots\n",
    "                ax1, ax2 = customize_plot(ax1, ax2, fontsize)\n",
    "                ax1.set_xlim(right=xlim)\n",
    "    \n",
    "                # Row 1: extracted mechanosorptive strain\n",
    "                if row == 0:\n",
    "                    ax1.plot(t_full, eps_full, '-', color=color, linewidth=linewidth)\n",
    "\n",
    "                # Row 2: fitted mechanosorptive strain\n",
    "                elif row == 1:\n",
    "                    # Reference grid\n",
    "                    t_new, w_new, s_new = build_cycles_with_initial(tdata0, wdata0, tdata, wdata, total_cycles=11, stress_value=s_full[0], stressed_cycles=9)\n",
    "                    eps_model_ref = mechanosorptive_model(comp_opt, s_new, t_new, w_new)\n",
    "                    eps_fit_list.append(eps_model_ref)\n",
    "                    ax1.plot(t_new, eps_model_ref, '-', color=color, linewidth=linewidth)\n",
    "\n",
    "                # Row 3: fitted mechanosorptive compliance\n",
    "                elif row == 2:\n",
    "                    # Reference grid\n",
    "                    t_new, w_new, s_new = build_cycles_with_initial(tdata0, wdata0, tdata, wdata, total_cycles=9, stress_value=s_full[0], stressed_cycles=9)\n",
    "                    eps_fit = mechanosorptive_model(comp_opt, s_new, t_new, w_new)\n",
    "                    eps_fit_list.append(eps_fit)\n",
    "                    s_list.append(s_new)\n",
    "                    Mc = np.where(s_new != 0, eps_fit / s_new, np.nan) * 1000.0  # [1/GPa]\n",
    "                    ax1.plot(t_new, Mc, '-', color=color, linewidth=linewidth)\n",
    "\n",
    "                # Row 4: normalized mechanosorptive compliance\n",
    "                else:\n",
    "                    # Reference grid\n",
    "                    t_new, w_new, s_new = build_cycles_with_initial(tdata0, wdata0, tdata, wdata, total_cycles=9, stress_value=s_full[0], stressed_cycles=9)\n",
    "                    eps_fit = mechanosorptive_model(comp_opt, s_new, t_new, w_new)\n",
    "                    eps_fit_list.append(eps_fit)\n",
    "                    s_list.append(s_new)\n",
    "\n",
    "            if row == 1:\n",
    "                if len(eps_fit_list):\n",
    "                    # Trimmed-mean\n",
    "                    exp_eps_ms = trim_mean(eps_fit_list, 0.3, axis=0)\n",
    "                    def resid_mean(cj):\n",
    "                        return mechanosorptive_model(cj, s_new, t_new, w_new) - exp_eps_ms\n",
    "                    res_mean = least_squares(resid_mean, np.array([0.01, 0.01, 0.01], float), bounds=(0, np.inf), xtol=1e-12, ftol=1e-12)\n",
    "                    comp_mean = res_mean.x\n",
    "                    eps_ms_mean = mechanosorptive_model(comp_mean, s_new, t_new, w_new)\n",
    "                    ax1.plot(t_new, eps_ms_mean, '--', color='black', linewidth=linewidth)\n",
    "            \n",
    "            elif row == 2:\n",
    "                if len(eps_fit_list):\n",
    "                    # Trimmed-mean\n",
    "                    exp_eps_ms = trim_mean(eps_fit_list, 0.3, axis=0)\n",
    "                    def resid_mean(cj):\n",
    "                        return mechanosorptive_model(cj, s_new, t_new, w_new) - exp_eps_ms\n",
    "                    res_mean = least_squares(resid_mean, np.array([0.01, 0.01, 0.01], float),bounds=(0, np.inf), xtol=1e-12, ftol=1e-12)\n",
    "                    comp_mean = res_mean.x\n",
    "                    eps_ms_mean = mechanosorptive_model(comp_mean, s_new, t_new, w_new)\n",
    "                    s_mean = np.mean(s_list)#, axis=0)\n",
    "                    Mc_mean = eps_ms_mean / s_mean * 1000.0\n",
    "                    ax1.plot(t_new, Mc_mean, '--', color='black', linewidth=linewidth)\n",
    "\n",
    "            elif row == 3:\n",
    "                if len(eps_fit_list):\n",
    "                    # Trimmed-mean\n",
    "                    exp_eps_ms = trim_mean(eps_fit_list, 0.3, axis=0)\n",
    "                    def resid_mean(cj):\n",
    "                        return mechanosorptive_model(cj, s_new, t_new, w_new) - exp_eps_ms\n",
    "                    res_mean = least_squares(resid_mean, np.array([0.01, 0.01, 0.01], float),bounds=(0, np.inf), xtol=1e-12, ftol=1e-12)\n",
    "                    comp_mean = res_mean.x\n",
    "                    eps_ms_mean = mechanosorptive_model(comp_mean, s_new, t_new, w_new)\n",
    "                    s_mean = np.mean(s_list)#, axis=0)\n",
    "                    Mc_mean = eps_ms_mean / s_mean\n",
    "\n",
    "                    # Calculate elastic compliance\n",
    "                    a, b, c = comp_el[sample_type]\n",
    "                    C_i = a * w_ref**2 + b * w_ref + c\n",
    "                    # Plot normalized compliance\n",
    "                    ax1.plot(t_new, Mc_mean/C_i, '-', color=rep_colors[ld_val], linewidth=linewidth)\n",
    "\n",
    "        # Customize axes\n",
    "        if row == 0:\n",
    "            # Plot moisture cycles\n",
    "            t_new, w_new, s_new = build_cycles_with_initial(tdata0, wdata0, tdata, wdata, total_cycles=11, stress_value=s_full[0], stressed_cycles=9)\n",
    "            ax2.plot(t_new, w_new, linestyle='--', color='grey', label=r'$\\omega$', linewidth=linewidth, alpha=0.5)\n",
    "            ax1.locator_params(axis='x', nbins=5)\n",
    "            if sample_type == 'RL': ax_ref = ax1\n",
    "            ax1.set_xticklabels([])\n",
    "            ax1.set_xlabel('')\n",
    "            ax1.set_ylim(top=ylim_row1)\n",
    "            ax1.locator_params(axis='y', nbins=4)\n",
    "            ax2.set_ylim(bottom=0.07, top=0.2)\n",
    "            set_shared_grid(ax1, ax2)\n",
    "            ax2.yaxis.set_major_formatter(mticker.FormatStrFormatter('%.2f'))\n",
    "\n",
    "            if sample_type in ('RT', 'TR'):\n",
    "                ax1.set_ylabel('')\n",
    "                ax1.set_yticklabels([])\n",
    "            else:\n",
    "                ax1.set_ylabel(r'$\\varepsilon^{ms}$ [-]', fontsize=fontsize)\n",
    "            if sample_type == 'TR': ax2.set_ylabel(r'$\\omega$ [-]', fontsize=fontsize, rotation=270, labelpad=32)\n",
    "            if sample_type in ('RT', 'RL'): ax2.set_yticklabels([])\n",
    "            if sample_type == 'RT': ax_leg = ax1\n",
    "            \n",
    "\n",
    "            # Set title\n",
    "            ax1.set_title(sample_type, fontsize=fontsize)\n",
    "\n",
    "        elif row == 1:\n",
    "            match_tick_values(ax_ref, ax1, axis='y', copy_labels=True, copy_limits=True)\n",
    "            match_tick_values(ax_ref, ax1, axis='x', copy_labels=True, copy_limits=True)\n",
    "            ax1.set_ylim(top=ylim_row2)\n",
    "            ax1.set_xlabel('')\n",
    "            ax1.set_xticklabels([])\n",
    "            ax2.set_yticks([])\n",
    "            if sample_type in ('RT', 'TR'):\n",
    "                ax1.set_ylabel('')\n",
    "                ax1.set_yticklabels([])\n",
    "            else:\n",
    "                ax1.set_ylabel(r'$\\varepsilon^{ms}_{fit}$ [-]', fontsize=fontsize)\n",
    "\n",
    "        elif row == 2:\n",
    "            match_tick_values(ax_ref, ax1, axis='x', copy_labels=True, copy_limits=True)\n",
    "            ax1.set_ylim(top=ylim_row3)\n",
    "            match_tick_count(ax_ref, ax1, axis='y')\n",
    "            ax1.set_xlabel('')\n",
    "            ax1.set_xticklabels([])\n",
    "            ax2.set_yticks([])\n",
    "            if sample_type == 'RT' or sample_type == 'TR':\n",
    "                ax1.set_ylabel('')\n",
    "                ax1.set_yticklabels([])\n",
    "            else:\n",
    "                ax1.set_ylabel(r'$M_c$ [1/GPa]', fontsize=fontsize)\n",
    "\n",
    "        else:\n",
    "            match_tick_values(ax_ref, ax1, axis='x', copy_labels=True, copy_limits=True)\n",
    "            ax1.set_xticklabels(ax1.get_xticks())\n",
    "            ax1.xaxis.set_major_formatter(mticker.FormatStrFormatter('%d'))\n",
    "            ax1.set_ylim(top=ylim_row4)\n",
    "            match_tick_count(ax_ref, ax1, axis='y')\n",
    "            ax2.set_yticks([])\n",
    "            if sample_type == 'RT' or sample_type == 'TR':\n",
    "                ax1.set_ylabel('')\n",
    "                ax1.set_yticklabels([])\n",
    "            else:\n",
    "                ax1.set_ylabel(r'$\\overline{M_c}/C_0^{-1}$ [-]', fontsize=fontsize)\n",
    "\n",
    "# Customize legend\n",
    "legend_handles = []\n",
    "for ld, color in rep_colors.items():\n",
    "    legend_handles.append(Line2D([0], [0], color=color, lw=linewidth, label=f'{int(ld*100)}% LD'))\n",
    "legend_handles = sorted(legend_handles, key=lambda h: int(h.get_label().replace('% LD', '')))\n",
    "legend_handles.append(Line2D([0], [0], color='black', lw=linewidth, linestyle='--', label='Mean fit'))\n",
    "legend_handles.append(Line2D([0], [0], color='grey', lw=linewidth, linestyle='--', label=r'$\\omega$', alpha=0.5))\n",
    "\n",
    "if ax_leg is not None and last_ax2 is not None:\n",
    "    ax_leg.set_zorder(last_ax2.get_zorder() + 1)\n",
    "    ax_leg.patch.set_alpha(0)\n",
    "    leg = ax_leg.legend(handles=legend_handles, fontsize=fontsize-2, loc='upper left')\n",
    "    leg.set_zorder = (1000)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot mean fitted mechanosorptive strain rate\n",
    "\n",
    "Run the following section to plot the evolution of mean fitted mechanosorptive strain rate $\\Delta\\overline{\\varepsilon}^{ms}_{fit}/\\Delta t$, grouped by **sample type** **{RL, RT, TR}** and **loading degree (LD)**, together with moisture cycles $\\omega$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## CALCULATE AND PLOT MS STRAIN RATE ##################\n",
    "# Prepare plot\n",
    "ylim_top = 0.012\n",
    "ylim_bott = -0.006\n",
    "fig, axes = plt.subplots(1, len(sample_types), figsize=(7 * len(sample_types), 6), constrained_layout=True)\n",
    "\n",
    "# Loop over each sample type\n",
    "for ax1, sample_type in zip(axes, sample_types):\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    # Subset for this sample_type\n",
    "    entries = [e for e in dataset_norm if e['sample_type'] == sample_type]\n",
    "    grouped, colors_by_ld = group_and_colors(entries, cmap_dict)\n",
    "\n",
    "    for ld_val, group in grouped.items():\n",
    "        eps_fit_list = []\n",
    "        s_list = []\n",
    "        for entry, color in zip(group, colors_by_ld[ld_val]):\n",
    "            # Retrieve datasets\n",
    "            w_full = entry['w_full'] # [-]\n",
    "            t_full = entry['t_full'] # [h]\n",
    "            s_full = entry['s_full'] # [MPa]\n",
    "            eps_full = entry['eps_full'] # [-]\n",
    "            comp_opt = entry['meta'].get('comp_opt', None)\n",
    "\n",
    "            # Reference grid\n",
    "            t_new, w_new, s_new = build_cycles_with_initial(tdata0, wdata0, tdata, wdata, total_cycles=11, stress_value=s_full[0], stressed_cycles=9)\n",
    "            eps_model_ref = mechanosorptive_model(comp_opt, s_new, t_new, w_new)\n",
    "            eps_fit_list.append(eps_model_ref)\n",
    "            \n",
    "        if len(eps_fit_list):\n",
    "            # Trimmed-mean\n",
    "            exp_eps_ms = trim_mean(eps_fit_list, 0.3, axis=0)\n",
    "            def resid_mean(cj):\n",
    "                return mechanosorptive_model(cj, s_new, t_new, w_new) - exp_eps_ms\n",
    "            res_mean = least_squares(resid_mean, np.array([0.01, 0.01, 0.01], float), bounds=(0, np.inf), xtol=1e-12, ftol=1e-12)\n",
    "            comp_mean = res_mean.x\n",
    "            eps_ms_mean = mechanosorptive_model(comp_mean, s_new, t_new, w_new)\n",
    "        \n",
    "        # Build strain‐rate array (time)\n",
    "        dt = np.diff(t_new)\n",
    "        dw = np.diff(w_new)\n",
    "        deps = np.diff(eps_ms_mean)\n",
    "        eps_t_rate = deps / dt\n",
    "        t_filt = t_new[1:]\n",
    "        # Filter out spikes\n",
    "        idx = detect_single_point_spikes(eps_t_rate, k=5)\n",
    "        eps_t_rate = fix_spikes_linear(eps_t_rate, idx)\n",
    "        ax1.plot(t_filt, eps_t_rate, '-', color=rep_colors[ld_val], linewidth=linewidth)\n",
    "\n",
    "    # Plot moisture\n",
    "    ax2.plot(t_new, w_new, linestyle='--', color='grey', label=r'$\\omega$', linewidth = linewidth, alpha=0.5)\n",
    "    # Customize axes\n",
    "    ax1, ax2 = customize_plot(ax1, ax2, fontsize)\n",
    "    match_tick_values(ax_ref, ax1, axis='x', copy_labels=True, copy_limits=True)\n",
    "    ax1.set_xticklabels(ax1.get_xticks())\n",
    "    ax1.xaxis.set_major_formatter(mticker.FormatStrFormatter('%d'))\n",
    "    ax1.set_ylim(bottom=ylim_bott, top=ylim_top)\n",
    "    match_tick_count(ax_ref, ax1, axis='y')\n",
    "    ax1.set_xlim(right=xlim)\n",
    "    ax2.set_ylim(bottom = 0.07, top=0.2)\n",
    "    set_shared_grid(ax1, ax2)\n",
    "    ax2.yaxis.set_major_formatter(mticker.FormatStrFormatter('%.2f'))\n",
    "    if sample_type == 'TR': ax2.set_ylabel(r'$\\omega$ [-]', fontsize=fontsize, rotation=270, labelpad=32)\n",
    "    if sample_type == 'RT' or sample_type == 'RL': ax2.set_yticklabels([])\n",
    "    if sample_type == 'RT' or sample_type == 'TR':\n",
    "        ax1.set_ylabel('')\n",
    "        ax1.set_yticklabels([])\n",
    "    else:\n",
    "        ax1.set_ylabel(r'$\\Delta\\overline{\\varepsilon}^{{ms}}_{{fit}} / \\Delta t$', fontsize=fontsize)\n",
    "    if sample_type == 'RT': ax_leg = ax1\n",
    "    # Customize title\n",
    "    ax1.set_title(sample_type, fontsize=fontsize)\n",
    "\n",
    "# Customize legend\n",
    "legend_handles = []\n",
    "for ld, color in rep_colors.items():\n",
    "    legend_handles.append(Line2D([0], [0], color=color, lw=linewidth, label=f'{int(ld*100)}% LD'))\n",
    "legend_handles = sorted(legend_handles, key=lambda h: int(h.get_label().replace('% LD', '')))\n",
    "legend_handles.append(Line2D([0], [0], color='grey', lw=linewidth, linestyle='--', label=r'$\\omega$', alpha=0.5))\n",
    "ax_leg.set_zorder(ax2.get_zorder() + 1)\n",
    "ax_leg.patch.set_alpha(0)\n",
    "leg = ax_leg.legend(handles=legend_handles, fontsize=fontsize-2, loc='upper right')\n",
    "leg.set_zorder=(1000)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
